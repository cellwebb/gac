#!/usr/bin/env python3
"""Workflow module for GAC."""

import logging
import os
import shutil
import subprocess
import sys
from datetime import datetime
from typing import Any, Dict, List, Optional, Tuple, Union

import click
from aisuite import Client
from aisuite.provider import Provider

from gac.ai_utils import chat, count_tokens
from gac.cache import Cache
from gac.config import get_config
from gac.errors import (
    AIConnectionError,
    AIModelError,
    AIRateLimitError,
    AITimeoutError,
    CacheError,
    GACError,
    GitError,
    convert_exception,
    handle_error,
)
from gac.formatting_controller import FormattingController
from gac.git import (
    commit_changes,
    get_project_description,
    get_staged_diff,
    get_staged_files,
    stage_files,
)
from gac.prompts import build_prompt, clean_commit_message, create_abbreviated_prompt
from gac.token_manager import TokenManager
from gac.utils import (
    print_error,
    print_header,
    print_info,
    print_success,
    print_warning,
    run_subprocess,
)

logger = logging.getLogger(__name__)


# Function to maintain compatibility with tests
def send_to_llm(prompt, model=None, api_key=None, max_tokens_to_sample=4096):
    """
    Send prompt to LLM (compatibility function for tests).

    Args:
        prompt: Prompt to send to model
        model: Model to use
        api_key: API key to use
        max_tokens_to_sample: Maximum tokens to sample in response

    Returns:
        Model response text
    """
    config = get_config()
    if not model:
        model = config.get("model")

    workflow = CommitWorkflow(
        verbose=False,
        quiet=False,
        test=False,
        formatting=False,
        add_all=False,
        model_override=model,
    )
    return workflow._send_to_llm(prompt, max_tokens_to_sample)


class CommitWorkflow:
    """Class that manages the Git commit workflow."""

    def __init__(
        self,
        test_mode: bool = False,
        force: bool = False,
        add_all: bool = False,
        no_format: bool = False,
        quiet: bool = False,
        verbose: bool = False,
        model: Optional[str] = None,
        one_liner: bool = False,
        show_prompt: bool = False,
        show_prompt_full: bool = False,
        test_with_real_diff: bool = False,
        hint: str = "",
        conventional: bool = False,
        no_cache: bool = False,
        clear_cache: bool = False,
        no_spinner: bool = False,
    ):
        """
        Initialize the CommitWorkflow.

        Args:
            test_mode: Run in test mode without making git commits
            force: Skip all confirmation prompts
            add_all: Stage all changes before committing
            no_format: Skip formatting of staged files
            quiet: Suppress non-error output
            verbose: Enable verbose logging
            model: Override the default model
            one_liner: Generate a single-line commit message
            show_prompt: Show an abbreviated version of the prompt
            show_prompt_full: Show the complete prompt
            test_with_real_diff: Test with real staged changes
            hint: Additional context to include in the prompt
            conventional: Generate a conventional commit format message
            no_cache: Skip cache and force fresh API calls
            clear_cache: Clear all cached data before running
            no_spinner: Disable progress spinner during API calls
        """
        self.test_mode = test_mode
        self.force = force
        self.add_all = add_all
        self.no_format = no_format
        self.quiet = quiet
        self.verbose = verbose
        self.model_override = model
        self.one_liner = one_liner
        self.show_prompt = show_prompt
        self.show_prompt_full = show_prompt_full
        self.test_with_real_diff = test_with_real_diff
        self.hint = hint
        self.conventional = conventional
        self.no_cache = no_cache
        self.clear_cache = clear_cache
        self.no_spinner = no_spinner

        # Initialize cache and formatting controller
        self.cache = Cache()
        self.formatting = FormattingController()

        # Get configuration
        self.config = get_config()
        if self.model_override:
            self.config["model"] = self.model_override

        # Configure logging
        if self.verbose:
            logger.setLevel(logging.DEBUG)
        else:
            logger.setLevel(logging.ERROR)

    def run(self) -> None:
        """Execute the full commit workflow."""
        try:
            # Clear cache if requested
            if self.clear_cache:
                self._clear_cache()

            # Prepare the commit (stage files if needed)
            self.prepare_commit()

            # Format staged files if formatting is enabled
            if not self.no_format and self.config.get("use_formatting", True):
                self.format_files()

            # Generate commit message
            commit_message = self.generate_message()
            if not commit_message:
                logger.error("No commit message was generated")
                if not self.quiet:
                    print_error("Failed to generate a commit message")
                return

            # Execute the commit
            if not self.test_mode:
                self.execute_commit(commit_message)
        except Exception as e:
            error = convert_exception(e, GACError, "An error occurred during the commit workflow")
            handle_error(error, quiet=self.quiet)

    def _clear_cache(self) -> None:
        """Clear all cached data."""
        try:
            cache_dir = os.path.expanduser("~/.cache/gac")
            if os.path.exists(cache_dir):
                if not self.quiet:
                    print_info("Clearing cache...")
                shutil.rmtree(cache_dir)
                if not self.quiet:
                    print_success("Cache cleared")
            self.cache.clear()
        except Exception as e:
            error = convert_exception(e, CacheError, "Failed to clear cache")
            handle_error(error, quiet=self.quiet, exit_program=False)

    def prepare_commit(self) -> None:
        """Prepare the commit by staging files if needed."""
        try:
            if self.add_all:
                if not self.quiet:
                    print_info("Staging all changes...")
                stage_files()
                if not self.quiet:
                    print_success("All changes staged")
        except Exception as e:
            error = convert_exception(e, GitError, "Failed to stage files")
            handle_error(error, quiet=self.quiet)

    def format_files(self) -> None:
        """Format staged files using the FormattingController."""
        staged_files = get_staged_files()
        if not staged_files:
            if not self.quiet:
                print_warning("No staged files to format")
            return

        self.formatting.format_staged_files(staged_files, self.quiet)

    def generate_message(self) -> str:
        """Generate a commit message using the LLM."""
        if not self.quiet:
            print_header("ðŸ¤– Generating commit message...")

        # Get staged diff
        try:
            if self.test_mode and not self.test_with_real_diff:
                # Use a sample diff for testing
                status = "On branch main\nChanges to be committed:\n  modified: README.md\n"
                diff = (
                    "diff --git a/README.md b/README.md\nindex 1234567..abcdefg 100644\n"
                    "--- a/README.md\n+++ b/README.md\n@@ -1,5 +1,5 @@\n # GAC\n"
                    "-Git Auto Commit\n+Git Auto Commit - AI powered commit messages\n"
                )
            else:
                status = run_subprocess(["git", "status", "--porcelain", "--branch"], trim=False)
                diff = get_staged_diff()

            if not diff.strip():
                if not self.quiet:
                    print_warning("No staged changes. Please stage your changes first.")
                return ""

            # Send to LLM
            commit_message = self._send_to_llm(
                status=status,
                diff=diff,
                one_liner=self.one_liner,
                show_prompt=self.show_prompt,
                show_prompt_full=self.show_prompt_full,
                hint=self.hint,
                force=self.force,
                conventional=self.conventional,
                cache_skip=self.no_cache,
                show_spinner=not self.no_spinner,
            )

            if commit_message and not self.quiet:
                print_success("Commit message generated:")
                print(commit_message)

            return commit_message
        except Exception as e:
            error = convert_exception(e, GACError, "Failed to generate commit message")
            handle_error(error, quiet=self.quiet, exit_program=False)
            return ""

    def execute_commit(self, commit_message: str) -> None:
        """Execute the commit with the generated message."""
        if not self.quiet:
            print_header("ðŸ’¾ Committing changes...")

        try:
            commit_changes(commit_message)
            if not self.quiet:
                print_success("Changes committed successfully!")
        except Exception as e:
            error = convert_exception(e, GitError, "Failed to commit changes")
            handle_error(error, quiet=self.quiet)

    def _send_to_llm(
        self,
        status: str,
        diff: str,
        one_liner: bool = False,
        show_prompt: bool = False,
        show_prompt_full: bool = False,
        hint: str = "",
        force: bool = False,
        conventional: bool = False,
        cache_skip: bool = False,
        show_spinner: bool = True,
    ) -> str:
        """
        Send the git status and staged diff to an LLM for summarization.

        Args:
            status: Output of git status
            diff: Output of git diff --staged
            one_liner: If True, request a single-line commit message
            show_prompt: If True, display an abbreviated version of the prompt
            show_prompt_full: If True, display the complete prompt with full diff
            hint: Optional context to include in the prompt (like "JIRA-123")
            force: If True, skip confirmation prompts
            conventional: If True, request a conventional commit format message
            cache_skip: If True, bypass cache and force a new API call
            show_spinner: If True, display a spinner during API calls

        Returns:
            The generated commit message
        """
        try:
            model = self.model_override or self.config.get("model")
            temperature = float(self.config.get("temperature", 0.7))
            api_key = self.config.get("api_key")

            # Parse provider and model
            provider_name = "anthropic"
            model_name = model
            if ":" in model:
                provider_name, model_name = model.split(":", 1)

            # Create provider config with the API key
            provider_configs = {provider_name: {"api_key": api_key}}

            # Initialize client
            client = Client(provider_configs=provider_configs)

            # Create completion request
            completion = client.complete(
                prompt=build_prompt(status, diff, one_liner, hint, conventional),
                provider=Provider(provider_name),
                model=model_name,
                max_tokens=4096,
                temperature=temperature,
            )

            return completion.text
        except Exception as e:
            error = convert_exception(e, GACError, "Failed to connect to the AI service")
            handle_error(error, quiet=self.quiet, exit_program=False)
            return ""
